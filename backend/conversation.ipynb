{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Other\\toolformer\\backend\\src\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Other\\toolformer\\backend\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mic\n",
    "import asyncio\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MIC] Starting listener\n",
      "[MIC] found microphone\n",
      "[MIC] Starting transcriber\n"
     ]
    }
   ],
   "source": [
    "stop_future: asyncio.Future = asyncio.get_running_loop().create_future()\n",
    "voice_queue=await mic.start_background(stop_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await voice_queue.get()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM inference is delegated to LLaMA-Adapter running on localhost\n",
    "https://github.com/TeamDman/LLaMA-Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_response(human_input):\n",
    "    url = \"http://127.0.0.1:5000/\"\n",
    "#     prompt = f\"\"\"The following is a conversation between a human and an AI.\n",
    "# Human: {human_input}\n",
    "# AI:\"\"\"\n",
    "    prompt=f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{human_input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    data = {\"prompts\": [prompt]}\n",
    "    response = requests.post(url, json=data).json()[0]\n",
    "    response = response.replace(prompt, \"\")\n",
    "    if \"Human:\" in response:\n",
    "        response = response.split(\"Human:\")[0]\n",
    "    response = response.strip()\n",
    "    return response\n",
    "# await get_response(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tts_models/en/blizzard2013/capacitron-t2-c50\n",
      " > tts_models/en/blizzard2013/capacitron-t2-c50 is already downloaded.\n",
      " > vocoder_models/en/blizzard2013/hifigan_v2 is already downloaded.\n",
      " > Using model: tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:80.0\n",
      " | > mel_fmax:12000.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:25.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 2\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:80.0\n",
      " | > mel_fmax:12000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:True\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "None\n",
      "speakers None\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "from IPython.display import Audio, display\n",
    "model_name = TTS.list_models()[20]\n",
    "print(model_name)\n",
    "tts = TTS(model_name, gpu=True)\n",
    "print(tts.speakers)\n",
    "print(\"speakers\", tts.speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    wav = tts.tts(text, speaker=None)\n",
    "    # display(Audio(wav, rate=tts.synthesizer.output_sample_rate))\n",
    "    tts.synthesizer.save_wav(wav=wav, path=\"output.wav\")\n",
    "    import winsound\n",
    "    winsound.PlaySound(\"output.wav\", winsound.SND_FILENAME)\n",
    "# speak(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_queue(q: asyncio.Queue):\n",
    "  while not q.empty():\n",
    "    # Depending on your program, you may want to\n",
    "    # catch QueueEmpty\n",
    "    q.get_nowait()\n",
    "    q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_queue(voice_queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Human: How do I, myself, my wife and Bigfoot, get water out of our ear and '\n",
      " 'put it into the ocean? What sauce do you recommend for this?')\n",
      "('AI: To get water out of your ear, you can use a bulb syringe or a cotton '\n",
      " 'swab to gently clean out the ear canal. To put the water into the ocean, you '\n",
      " 'can use a bucket or a watering can to collect the water and pour it into the '\n",
      " 'ocean. For the sauce, you can use a mixture of lemon juice and salt to help '\n",
      " 'clean the ear canal.')\n",
      " > Text splitted to sentences.\n",
      "['To get water out of your ear, you can use a bulb syringe or a cotton swab to gently clean out the ear canal.', 'To put the water into the ocean, you can use a bucket or a watering can to collect the water and pour it into the ocean.', 'For the sauce, you can use a mixture of lemon juice and salt to help clean the ear canal.']\n",
      "tə ɡɛt wˈɔːɾɚɹ ˌaʊɾəv jʊɹ ˈɪɹ, juː kæn jˈuːz ɐ bˈʌlb sɪɹˈɪndʒ ɔːɹ ɐ kˈɑːʔn̩ swˈɑːb tə dʒˈɛntli klˈiːn ˈaʊt ðɪ ˈɪɹ kənˈæl.\n",
      " [!] Character '̩' not found in the vocabulary. Discarding it.\n",
      " > Processing time: 1.740370750427246\n",
      " > Real-time factor: 0.12405817258189751\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "empty_queue(voice_queue)\n",
    "\n",
    "# while True:\n",
    "human = (await voice_queue.get()).strip()\n",
    "if human != \"\":\n",
    "    pprint(f\"Human: {human}\")\n",
    "    response = await get_response(human)\n",
    "    pprint(f\"AI: {response}\")\n",
    "    speak(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
