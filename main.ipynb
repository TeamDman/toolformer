{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718f08c5e164f0aa5f9ec3e6c594125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Repos\\Other\\toolformer\\pythia-6.9B-deduped\\step143000. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67883da2591644b0acd10c6aa113d36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fb04c42ad546fa87b83c091d88e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1fc95029294c50a29eba164519ba08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 23.99 GiB total capacity; 22.89 GiB already allocated; 0 bytes free; 22.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPTNeoXForCausalLM, AutoTokenizer, PreTrainedTokenizerFast\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m GPTNeoXForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m   \u001b[39m\"\u001b[39;49m\u001b[39mEleutherAI/pythia-6.9B-deduped\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m   revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep143000\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m   cache_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./pythia-6.9B-deduped/step143000\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m----> 7\u001b[0m )\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1132\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1129\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1132\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:784\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    783\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 784\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    786\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    787\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    788\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    789\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    795\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:784\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    783\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 784\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    786\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    787\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    788\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    789\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    795\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 784 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:784\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    783\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 784\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    786\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    787\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    788\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    789\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    795\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:807\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 807\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    808\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    809\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1128\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1129\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1130\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 23.99 GiB total capacity; 22.89 GiB already allocated; 0 bytes free; 22.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8B-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-2.8B-deduped/step143000\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-2.8B-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-2.8B-deduped/step143000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_added_toks = tokenizer.add_tokens([\"[\",\"]\",\"->\"], special_tokens=True)\n",
    "print(num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50277, 2560)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens) \n",
    "# doesn't show our new ones for some reason /shrug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  262,   310,   209,    60, 22731,  1082,   209,  1168, 15567,    62],\n",
      "       device='cuda:0')\n",
      "['it', 'Ġis', 'Ġ', '[', 'NOW', '()', 'Ġ', '->', 'Ġ123', ']']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"it is [NOW() -> 123]\", return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"][0]\n",
    "print(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(65924, '', 'explorer.exe'),\n",
       " (66074, '', 'explorer.exe'),\n",
       " (66052, '', 'explorer.exe'),\n",
       " (2689676, '● main.ipynb - toolformer - Visual Studio Code', 'Code.exe'),\n",
       " (327782, 'Task Manager', 'Taskmgr.exe'),\n",
       " (395466,\n",
       "  '(14) Beethoven - Moonlight Sonata (FULL) - YouTube and 2 more pages - Personal - Microsoft\\u200b Edge Dev',\n",
       "  'msedge.exe'),\n",
       " (396024, 'Ideas - Visual Studio Code', 'Code.exe'),\n",
       " (132436, '@Chase - Discord', 'Discord.exe'),\n",
       " (133276, '', 'explorer.exe'),\n",
       " (67736, 'Clock', 'Time.exe'),\n",
       " (132478, 'Clock', 'ApplicationFrameHost.exe'),\n",
       " (132608, 'Groove Music', 'Music.UI.exe'),\n",
       " (328706, 'Groove Music', 'ApplicationFrameHost.exe'),\n",
       " (787954, 'Settings', 'SystemSettings.exe'),\n",
       " (198766, 'Settings', 'ApplicationFrameHost.exe'),\n",
       " (66224, '', 'explorer.exe'),\n",
       " (66216, '', 'explorer.exe'),\n",
       " (66198, '', 'explorer.exe'),\n",
       " (66180, '', 'explorer.exe'),\n",
       " (66150, '', 'explorer.exe'),\n",
       " (66148, '', 'explorer.exe'),\n",
       " (262364, 'Microsoft Text Input Application', 'TextInputHost.exe'),\n",
       " (66106, 'Program Manager', 'explorer.exe')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_windows():\n",
    "    import psutil, win32process, win32gui, time\n",
    "    def get_process_name(handle):\n",
    "        pid = win32process.GetWindowThreadProcessId(handle) #This produces a list of PIDs active window relates to\n",
    "        return (psutil.Process(pid[-1]).name()) #pid[-1] is the most likely to survive last longer\n",
    "\n",
    "    rtn = []\n",
    "    def winEnumHandler( hwnd, ctx ):\n",
    "        nonlocal rtn\n",
    "        if win32gui.IsWindowVisible( hwnd ):\n",
    "            rtn.append((hwnd, win32gui.GetWindowText( hwnd ), get_process_name(hwnd)))\n",
    "    win32gui.EnumWindows( winEnumHandler, None )\n",
    "    return rtn\n",
    "list_windows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396024, 'Ideas - Visual Studio Code', 'Code.exe')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\gptj\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_match(name):\n",
    "    from fuzzywuzzy import fuzz\n",
    "    x = sorted([(fuzz.ratio(str(x), name),x) for x in list_windows()], key=lambda x: x[0], reverse=True)\n",
    "    print(x[0][1])\n",
    "    return x[0][1][0]\n",
    "get_best_match(\"VSCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 540, 960, 540)\n",
      "(0, 0, 1920, 1080)\n"
     ]
    }
   ],
   "source": [
    "def pos2pos(pos: str) -> (int,int,int,int):\n",
    "    from screeninfo import get_monitors\n",
    "    monitors = get_monitors()\n",
    "    left = sorted(monitors, key=lambda x: x.x)[0]\n",
    "    right = sorted(monitors, key=lambda x: x.x, reverse=True)[0]\n",
    "    main = [x for x in monitors if x.is_primary][0]\n",
    "    corners = {\n",
    "        \"top left\": lambda m: (m.x, m.y, m.width//2, m.height//2),\n",
    "        \"top right\": lambda m: (m.x + m.width//2, m.y, m.width//2, m.height//2),\n",
    "        \"bottom left\": lambda m: (m.x, m.y + m.height//2, m.width//2, m.height//2),\n",
    "        \"bottom right\": lambda m: (m.x + m.width//2, m.y + m.height//2, m.width//2, m.height//2),\n",
    "    }\n",
    "    lookup = {}\n",
    "    for corner, func in corners.items():\n",
    "        lookup[f\"left monitor, {corner}\"] = func(left)\n",
    "        lookup[f\"third monitor, {corner}\"] = func(left)\n",
    "        \n",
    "        lookup[f\"main monitor, {corner}\"] = func(main)\n",
    "\n",
    "        lookup[f\"right monitor, {corner}\"] = func(right)\n",
    "        lookup[f\"second monitor, {corner}\"] = func(right)\n",
    "        lookup[f\"secondary monitor, {corner}\"] = func(right)\n",
    "    \n",
    "    lookup[f\"main monitor, full\"] = (main.x, main.y, main.width, main.height)\n",
    "    \n",
    "    lookup[f\"left monitor, full\"] = (left.x, left.y, left.width, left.height)\n",
    "    lookup[f\"third monitor, full\"] = (left.x, left.y, left.width, left.height)\n",
    "    \n",
    "    lookup[f\"right monitor, full\"] = (right.x, right.y, right.width, right.height)\n",
    "    lookup[f\"secondary monitor, full\"] = (right.x, right.y, right.width, right.height)\n",
    "\n",
    "    from fuzzywuzzy import fuzz\n",
    "    x = sorted([(fuzz.ratio(label, pos),data) for label,data in lookup.items()], key=lambda x: x[0], reverse=True)\n",
    "    # print(x)\n",
    "    return x[0][1]\n",
    "    \n",
    "print(pos2pos(\"right monitor, bottom left\"))\n",
    "print(pos2pos(\"main monitor, full\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap(params):\n",
    "    window, position = params.split(\",\")\n",
    "\n",
    "    import ctypes\n",
    "    user32 = ctypes.windll.user32\n",
    "\n",
    "    user32.SetProcessDPIAware()\n",
    "    res = (user32.GetSystemMetrics(0), user32.GetSystemMetrics(1))\n",
    "    print(\"found resolution: \", res)\n",
    "    handle = get_best_match(window)\n",
    "    # handle = user32.FindWindowW(None, u'Untitled - Notepad')\n",
    "    if handle == 0:\n",
    "        return \"could not find window\"\n",
    "    # move window using handle\n",
    "    # MoveWindow(handle, x, y, height, width, repaint(bool))\n",
    "    pos = pos2pos(position)\n",
    "    user32.MoveWindow(handle, *pos, True)\n",
    "    return f\"{window} to {pos}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found resolution:  (1920, 1080)\n",
      "(328706, 'Groove Music', 'ApplicationFrameHost.exe')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'notepad to (1920, 540, 960, 540)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap(\"notepad, second monitor bottom left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_functions = {\n",
    "    \"SNAP\": lambda x: snap(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(response):\n",
    "    import re\n",
    "    pattern = r\".*\\[(\\w+)\\((.*?)\\s*\\)\\s*->\"\n",
    "    print(re.findall(pattern, response, re.DOTALL))\n",
    "    x = re.findall(pattern, response)\n",
    "    if len(x) == 0:\n",
    "        return \"bad match\"\n",
    "    func, params = x[0]\n",
    "\n",
    "    if func not in known_functions:\n",
    "        return f\"Unknown function {func}\"\n",
    "    print(f\"Calling {func} with {params}\")\n",
    "    result = known_functions[func](params)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt: str, eos: str) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        # do_sample=True,\n",
    "        temperature=0.9,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=tokenizer.encode(eos)[0],\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[:,inputs[\"input_ids\"].shape[1]:][0])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:62 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are using the wrong definition of the sum of a series.\n",
      "The sum of a series is defined as the limit of the partial sums.\n",
      "So, if you have a series $a_n$ with $a_n = \\frac{1}{n}$ for $n \\in \\mathbb{N}$, then the sum of this series is $\\sum_{n=1}^\\infty \\frac{1}{n} =\n",
      "tensor([[  746,    12,   740,    30,    17,    15,   187,   187,    34,    27,\n",
      "           187,   187,   510,  1895,   310,   326,   368,   403,   970,   253,\n",
      "          3430,  5426,   273,   253,  2020,   273,   247,  2962,    15,   187,\n",
      "           510,  2020,   273,   247,  2962,   310,  2931,   347,   253,  2701,\n",
      "           273,   253,  7898, 22661,    15,   187,  2598,    13,   604,   368,\n",
      "           452,   247,  2962,   370,    66,    64,    79,     5,   342,   370,\n",
      "            66,    64,    79,   426,   393,  1124,    92,    18,  1217,    79,\n",
      "           724,   323,   370,    79,   393,   249,   393,  1991,    92,    47,\n",
      "          3303,   840,   253,  2020,   273,   436,  2962,   310,   669,  2204,\n",
      "           578,    79,    30,    18,  9616,  3259,   393,  1124,    92,    18,\n",
      "          1217,    79,    94,   426]], device='cuda:0')\n",
      "19+10=0.\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are using the wrong definition of the sum of a series.\n",
      "The sum of a series is defined as the limit of the partial sums.\n",
      "So, if you have a series $a_n$ with $a_n = \\frac{1}{n}$ for $n \\in \\mathbb{N}$, then the sum of this series is $\\sum_{n=1}^\\infty \\frac{1}{n} =\n"
     ]
    }
   ],
   "source": [
    "response = predict(\"19+10=\", eos=\"]\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant with several tools available to you. The tools are the following:\n",
      "NOW: Print the current time.\n",
      "\n",
      "DO NOT USE TOOLS WITHIN TOOLS! KEEP ALL TOOL CALLS SEPARATE FROM EACH OTHER!\n",
      "\n",
      "User: What time is it?\n",
      "Assistant: It is [NOW() -> 2021-07-23 11:51:08.123456] 11:51 AM.\n",
      "User: What is the date?\n",
      "Assistant: It is [NOW() -> 2021-07-23 11:51:08.123456] 2021-07-23.\n",
      "User: \n"
     ]
    }
   ],
   "source": [
    "from tools.helpers import handle_tool_invocation, build_preprompt\n",
    "print(build_preprompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1168 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:187 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SNAP', 'Notepad, right monitor bottom right')]\n",
      "Calling SNAP with Notepad, right monitor bottom right\n",
      "found resolution:  (1920, 1080)\n",
      "(1183534, 'Untitled - Notepad', 'notepad.exe')\n",
      "You are an AI assistant with several tools available to you. The tools are the following:\n",
      "MATH: This allows you to evaluate mathematical expressions using the math.js library.\n",
      "NOW: This returns the current date and time. You must not pass any arguments to this tool!\n",
      "SEARCH: This returns a web search result for the given string argument.\n",
      "REMEMBER: This stores a key-value pair in the assistant's memory. You must pass two arguments to this tool, the first being the key, and the second being the value.\n",
      "RECALL: This returns a value from the assistant's memory. You must pass one argument to this tool, the key.\n",
      "OPEN: This opens a file or a website. The only argument is the URI for the file or website.\n",
      "SNAP: This moves a window to an area of the screen. The first argument is the program, and the second argument is the position.\n",
      "\n",
      "DO NOT USE TOOLS WITHIN TOOLS! KEEP ALL TOOL CALLS SEPARATE FROM EACH OTHER!\n",
      "\n",
      "User: What is 10 times 14?\n",
      "Assistant:10 * 14 is [MATH(10 * 14) -> 140] 140.\n",
      "User: Has Avatar the way of water been released yet?\n",
      "Assistant:[SEARCH(Avatar the way of water release date) -> 22.11.2022] Avatar: The way of water was released on the 22nd of november 2022. Today is [NOW() -> Mon Feb 27 2023] the 13th of February 2023. Therefore, [MATH(2023 > 2022) -> true] it was released last year.\n",
      "User: What day is it?\n",
      "Assistant:It is [NOW() -> 13/02/2023, 20:01:35] the 13th of February 2023.\n",
      "User: How old is Donald Trump?\n",
      "Assistant:Donald Trump is [SEARCH(Donald Trump age) -> 76] 76 years old.\n",
      "User: Where was the inventor of the light bulb born, and what is the last country his home country was at war with?\n",
      "Assistant:Thomas Edison, the inventor of the lightbulb, was born in [SEARCH(Thomas Edison birthplace) -> Milan, Ohio] Milan, Ohio. The last country the United States was at war with was [SEARCH(last country US at war with) -> Iraq] Iraq.\n",
      "User: Set my default browser to Firefox.\n",
      "Assistant:Okay, I set your default browser to [REMEMBER(default browser, Firefox) -> Firefox].\n",
      "User: Open youtube.\n",
      "Assistant: Okay, opening YouTube... [OPEN(https://youtube.com) -> 0] done!\n",
      "User: Open my csgo config.\n",
      "Assistant: Okay, opening your csgo config... [OPEN(C:\\Program Files (x86)\\Steam\\steamapps\\common\\Counter-Strike Global Offensive\\csgo\\cfg\\autoexec.cfg) -> 0] done!\n",
      "User: Move chrome to my left monitor.\n",
      "Assistant: Okay, moving chrome... [SNAP(Chrome, left monitor) -> 0] done!\n",
      "User: Move spotify to the top right corner of my second monitor.\n",
      "Assistant: Okay, moving spotify... [SNAP(Spotify, right monitor top right) -> 0] done!\n",
      "User: Move edge to the main monitor, top right.\n",
      "Assistant: Okay, moving edge... [SNAP(Edge, main monitor top right) -> 0] done!\n",
      "User: Move discord to my third monitor\n",
      "Assistant: Okay, moving discord... [SNAP(Discord, third monitor) -> 0] done!\n",
      "User: move terminal, secondary monitor, top right\n",
      "Assistant: Okay, moving terminal... [SNAP(Terminal, second monitor top right) -> 0] done!\n",
      "User: move notepad, main monitor\n",
      "Assistant: Okay, moving notepad... [SNAP(Notepad, main monitor full) -> 0] done!\n",
      "User: move notepad, right monitor, bottom right\n",
      "Assistant: Okay, moving notepad... [SNAP(Notepad, right monitor bottom right) ->Notepad to (2880, 540, 960, 540)]\n",
      " done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_prompt = r\"\"\"You are an AI assistant with several tools available to you. The tools are the following:\n",
    "MATH: This allows you to evaluate mathematical expressions using the math.js library.\n",
    "NOW: This returns the current date and time. You must not pass any arguments to this tool!\n",
    "SEARCH: This returns a web search result for the given string argument.\n",
    "REMEMBER: This stores a key-value pair in the assistant's memory. You must pass two arguments to this tool, the first being the key, and the second being the value.\n",
    "RECALL: This returns a value from the assistant's memory. You must pass one argument to this tool, the key.\n",
    "OPEN: This opens a file or a website. The only argument is the URI for the file or website.\n",
    "SNAP: This moves a window to an area of the screen. The first argument is the program, and the second argument is the position.\n",
    "\n",
    "DO NOT USE TOOLS WITHIN TOOLS! KEEP ALL TOOL CALLS SEPARATE FROM EACH OTHER!\n",
    "\n",
    "User: What is 10 times 14?\n",
    "Assistant:10 * 14 is [MATH(10 * 14) -> 140] 140.\n",
    "User: Has Avatar the way of water been released yet?\n",
    "Assistant:[SEARCH(Avatar the way of water release date) -> 22.11.2022] Avatar: The way of water was released on the 22nd of november 2022. Today is [NOW() -> Mon Feb 27 2023] the 13th of February 2023. Therefore, [MATH(2023 > 2022) -> true] it was released last year.\n",
    "User: What day is it?\n",
    "Assistant:It is [NOW() -> 13/02/2023, 20:01:35] the 13th of February 2023.\n",
    "User: How old is Donald Trump?\n",
    "Assistant:Donald Trump is [SEARCH(Donald Trump age) -> 76] 76 years old.\n",
    "User: Where was the inventor of the light bulb born, and what is the last country his home country was at war with?\n",
    "Assistant:Thomas Edison, the inventor of the lightbulb, was born in [SEARCH(Thomas Edison birthplace) -> Milan, Ohio] Milan, Ohio. The last country the United States was at war with was [SEARCH(last country US at war with) -> Iraq] Iraq.\n",
    "User: Set my default browser to Firefox.\n",
    "Assistant:Okay, I set your default browser to [REMEMBER(default browser, Firefox) -> Firefox].\n",
    "User: Open youtube.\n",
    "Assistant: Okay, opening YouTube... [OPEN(https://youtube.com) -> 0] done!\n",
    "User: Open my csgo config.\n",
    "Assistant: Okay, opening your csgo config... [OPEN(C:\\Program Files (x86)\\Steam\\steamapps\\common\\Counter-Strike Global Offensive\\csgo\\cfg\\autoexec.cfg) -> 0] done!\n",
    "User: Move chrome to my left monitor.\n",
    "Assistant: Okay, moving chrome... [SNAP(Chrome, left monitor) -> 0] done!\n",
    "User: Move spotify to the top right corner of my second monitor.\n",
    "Assistant: Okay, moving spotify... [SNAP(Spotify, right monitor top right) -> 0] done!\n",
    "User: Move edge to the main monitor, top right.\n",
    "Assistant: Okay, moving edge... [SNAP(Edge, main monitor top right) -> 0] done!\n",
    "User: Move discord to my third monitor\n",
    "Assistant: Okay, moving discord... [SNAP(Discord, third monitor) -> 0] done!\n",
    "User: move terminal, secondary monitor, top right\n",
    "Assistant: Okay, moving terminal... [SNAP(Terminal, second monitor top right) -> 0] done!\n",
    "User: move notepad, main monitor\n",
    "Assistant: Okay, moving notepad... [SNAP(Notepad, main monitor full) -> 0] done!\n",
    "User: \"\"\"\n",
    "\n",
    "# prompt = pre_prompt + \"move notepad, main monitor\"\n",
    "prompt = pre_prompt + input() + \"\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "# print(prompt)\n",
    "tokens = model.generate(\n",
    "    **inputs,\n",
    "    # do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_new_tokens=500,\n",
    "    eos_token_id=tokenizer.encode(\"->\")[0],\n",
    ")\n",
    "response = tokenizer.decode(tokens[:,inputs[\"input_ids\"].shape[1]:][0])\n",
    "output = invoke(response)\n",
    "prompt = prompt + response + output + \"]\"\n",
    "# print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(prompt)\n",
    "tokens = model.generate(\n",
    "    **inputs,\n",
    "    # do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=tokenizer.encode(\"\\n\")[0],\n",
    ")\n",
    "response = tokenizer.decode(tokens[:,inputs[\"input_ids\"].shape[1]:][0])\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e00874b6988a7999354735cded7abcfddd61bc52daa5a68bebc5283a04aa2a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
